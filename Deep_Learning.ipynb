{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Read the charity_data.csv into a Pandas DataFrame\n",
    "url = \"https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv\"\n",
    "df = pd.read_csv(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "y = df['IS_SUCCESSFUL']\n",
    "X = df.drop(['EIN', 'NAME', 'IS_SUCCESSFUL'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APPLICATION_TYPE value counts:\n",
      "APPLICATION_TYPE\n",
      "T3     27037\n",
      "T4      1542\n",
      "T6      1216\n",
      "T5      1173\n",
      "T19     1065\n",
      "T8       737\n",
      "T7       725\n",
      "T10      528\n",
      "T9       156\n",
      "T13       66\n",
      "T12       27\n",
      "T2        16\n",
      "T25        3\n",
      "T14        3\n",
      "T29        2\n",
      "T15        2\n",
      "T17        1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CLASSIFICATION value counts:\n",
      "CLASSIFICATION\n",
      "C1000    17326\n",
      "C2000     6074\n",
      "C1200     4837\n",
      "C3000     1918\n",
      "C2100     1883\n",
      "         ...  \n",
      "C4120        1\n",
      "C8210        1\n",
      "C2561        1\n",
      "C4500        1\n",
      "C2150        1\n",
      "Name: count, Length: 71, dtype: int64\n",
      "\n",
      "ASK_AMT value counts:\n",
      "ASK_AMT\n",
      "5000        25398\n",
      "10478           3\n",
      "15583           3\n",
      "63981           3\n",
      "6725            3\n",
      "            ...  \n",
      "5371754         1\n",
      "30060           1\n",
      "43091152        1\n",
      "18683           1\n",
      "36500179        1\n",
      "Name: count, Length: 8747, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Determine the number of unique values in each column.\n",
    "unique_values = X.nunique()\n",
    "\n",
    "# For columns with more than 10 unique values, determine the number of data points for each unique value.\n",
    "for column in X.columns:\n",
    "    if unique_values[column] > 10:\n",
    "        print(f\"{column} value counts:\\n{X[column].value_counts()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a cutoff value and create a list of categorical variables to be replaced with \"Other\"\n",
    "cutoff_value = 500\n",
    "columns_to_replace = []\n",
    "for column in X.columns:\n",
    "    if unique_values[column] > 10:\n",
    "        value_counts = X[column].value_counts()\n",
    "        columns_to_replace.extend(list(value_counts[value_counts < cutoff_value].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APPLICATION_TYPE value counts after binning:\n",
      "APPLICATION_TYPE\n",
      "T3       27037\n",
      "T4        1542\n",
      "T6        1216\n",
      "T5        1173\n",
      "T19       1065\n",
      "T8         737\n",
      "T7         725\n",
      "T10        528\n",
      "Other      276\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CLASSIFICATION value counts after binning:\n",
      "CLASSIFICATION\n",
      "C1000    17326\n",
      "C2000     6074\n",
      "C1200     4837\n",
      "C3000     1918\n",
      "C2100     1883\n",
      "Other     1484\n",
      "C7000      777\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ASK_AMT value counts after binning:\n",
      "ASK_AMT\n",
      "5000     25398\n",
      "Other     8901\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace in the dataframe\n",
    "for column in X.columns:\n",
    "    if unique_values[column] > 10:\n",
    "        X[column] = X[column].replace(columns_to_replace, \"Other\")\n",
    "\n",
    "# Check to make sure binning was successful\n",
    "for column in X.columns:\n",
    "    if unique_values[column] > 10:\n",
    "        print(f\"{column} value counts after binning:\\n{X[column].value_counts()}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pd.get_dummies() to encode categorical variables\n",
    "X_encoded = pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the preprocessed data into a features array, X, and a target array, y.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "# Scale the training and testing features datasets\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Design a neural network model\n",
    "nn = tf.keras.models.Sequential()\n",
    "nn.add(tf.keras.layers.Dense(units=80, input_dim=len(X_train.columns), activation='relu'))\n",
    "nn.add(tf.keras.layers.Dense(units=30, activation='relu'))\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "nn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    }
   ],
   "source": [
    "# Create a callback that saves the model's weights every five epochs\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"AlphabetSoupCharity.h5\", save_weights_only=True, save_best_only=True, monitor='accuracy', mode='max', verbose=2, period=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "WARNING:tensorflow:From c:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "804/804 [==============================] - 1s 918us/step - loss: 0.5704 - accuracy: 0.7227\n",
      "Epoch 2/25\n",
      "804/804 [==============================] - 1s 908us/step - loss: 0.5526 - accuracy: 0.7299\n",
      "Epoch 3/25\n",
      "804/804 [==============================] - 1s 895us/step - loss: 0.5501 - accuracy: 0.7321\n",
      "Epoch 4/25\n",
      "804/804 [==============================] - 1s 903us/step - loss: 0.5473 - accuracy: 0.7346\n",
      "Epoch 5/25\n",
      "782/804 [============================>.] - ETA: 0s - loss: 0.5463 - accuracy: 0.7351\n",
      "Epoch 5: accuracy improved from -inf to 0.73472, saving model to AlphabetSoupCharity.h5\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5468 - accuracy: 0.7347\n",
      "Epoch 6/25\n",
      "804/804 [==============================] - 1s 892us/step - loss: 0.5464 - accuracy: 0.7343\n",
      "Epoch 7/25\n",
      "804/804 [==============================] - 1s 892us/step - loss: 0.5451 - accuracy: 0.7353\n",
      "Epoch 8/25\n",
      "804/804 [==============================] - 1s 896us/step - loss: 0.5439 - accuracy: 0.7344\n",
      "Epoch 9/25\n",
      "804/804 [==============================] - 1s 906us/step - loss: 0.5439 - accuracy: 0.7362\n",
      "Epoch 10/25\n",
      "801/804 [============================>.] - ETA: 0s - loss: 0.5431 - accuracy: 0.7360\n",
      "Epoch 10: accuracy improved from 0.73472 to 0.73577, saving model to AlphabetSoupCharity.h5\n",
      "804/804 [==============================] - 1s 920us/step - loss: 0.5432 - accuracy: 0.7358\n",
      "Epoch 11/25\n",
      "804/804 [==============================] - 1s 887us/step - loss: 0.5428 - accuracy: 0.7367\n",
      "Epoch 12/25\n",
      "804/804 [==============================] - 1s 886us/step - loss: 0.5421 - accuracy: 0.7342\n",
      "Epoch 13/25\n",
      "804/804 [==============================] - 1s 882us/step - loss: 0.5419 - accuracy: 0.7361\n",
      "Epoch 14/25\n",
      "804/804 [==============================] - 1s 869us/step - loss: 0.5414 - accuracy: 0.7368\n",
      "Epoch 15/25\n",
      "785/804 [============================>.] - ETA: 0s - loss: 0.5412 - accuracy: 0.7373\n",
      "Epoch 15: accuracy improved from 0.73577 to 0.73760, saving model to AlphabetSoupCharity.h5\n",
      "804/804 [==============================] - 1s 920us/step - loss: 0.5407 - accuracy: 0.7376\n",
      "Epoch 16/25\n",
      "804/804 [==============================] - 1s 889us/step - loss: 0.5406 - accuracy: 0.7374\n",
      "Epoch 17/25\n",
      "804/804 [==============================] - 1s 903us/step - loss: 0.5397 - accuracy: 0.7374\n",
      "Epoch 18/25\n",
      "804/804 [==============================] - 1s 939us/step - loss: 0.5404 - accuracy: 0.7371\n",
      "Epoch 19/25\n",
      "804/804 [==============================] - 1s 911us/step - loss: 0.5394 - accuracy: 0.7381\n",
      "Epoch 20/25\n",
      "753/804 [===========================>..] - ETA: 0s - loss: 0.5397 - accuracy: 0.7386\n",
      "Epoch 20: accuracy improved from 0.73760 to 0.73869, saving model to AlphabetSoupCharity.h5\n",
      "804/804 [==============================] - 1s 975us/step - loss: 0.5396 - accuracy: 0.7387\n",
      "Epoch 21/25\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5393 - accuracy: 0.7377\n",
      "Epoch 22/25\n",
      "804/804 [==============================] - 1s 987us/step - loss: 0.5385 - accuracy: 0.7380\n",
      "Epoch 23/25\n",
      "804/804 [==============================] - 1s 1ms/step - loss: 0.5386 - accuracy: 0.7371\n",
      "Epoch 24/25\n",
      "804/804 [==============================] - 1s 931us/step - loss: 0.5382 - accuracy: 0.7397\n",
      "Epoch 25/25\n",
      "784/804 [============================>.] - ETA: 0s - loss: 0.5379 - accuracy: 0.7389\n",
      "Epoch 25: accuracy did not improve from 0.73869\n",
      "804/804 [==============================] - 1s 972us/step - loss: 0.5381 - accuracy: 0.7383\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1c5f123c410>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "nn.fit(X_train_scaled, y_train, epochs=25, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268/268 - 0s - loss: 0.5538 - accuracy: 0.7242 - 292ms/epoch - 1ms/step\n",
      "Loss: 0.5538214445114136, Accuracy: 0.7241982221603394\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled, y_test, verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sebastian\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save and export the results to an HDF5 file\n",
    "nn.save(\"AlphabetSoupCharity.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
